{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint /home/sayontan/.cache/huggingface/metrics/bleurt/default/downloads/extracted/98dc9460806ce3f1e4bb720f895eb85c10b0ce49c567cc7c70c9b108906be5cd/bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-06 06:13:42.816067: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2023-12-06 06:13:42.816138: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: context\n",
      "2023-12-06 06:13:42.816150: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: context\n",
      "2023-12-06 06:13:42.816316: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2023-12-06 06:13:42.816369: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.80.2\n",
      "2023-12-06 06:13:42.816714: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5TokenizerFast,\n",
    "    get_linear_schedule_with_warmup, \n",
    "    BertTokenizerFast, RobertaTokenizerFast)\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import gc\n",
    "gc.disable()\n",
    "import time\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from pasta_dataset import create_train_test_dataset\n",
    "\n",
    "bert_score = evaluate.load(\"bertscore\")\n",
    "bleu_score = evaluate.load(\"google_bleu\")\n",
    "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Train_data shape :: (8476, 20)\n",
      "test_data shape :: (917, 20)\n",
      "val_data shape :: (1350, 20)\n",
      "\\----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((8476, 20), (1350, 20), (917, 22))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = create_train_test_dataset(root = './../data')\n",
    "train_data, test_data, val_data = data_dict['tr_dat'], data_dict['te_dat'], data_dict['val_dat']\n",
    "\n",
    "test_data.loc[:, 'story'] = test_data.apply(lambda x: \" \".join([x[f'Input.line{i}'] for i in range(1, 6)]), axis = 1)\n",
    "test_data.loc[:, 'mod_story'] = test_data.apply(lambda x: \" \".join([x[f'Answer.mod_line{i}'] for i in range(1, 6)]), axis = 1)\n",
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 8 - Human performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(df, aid_subset = None):\n",
    "    df['correct_pred'] = df.apply(lambda x: int((str(x['target_op']).strip() == str(x['gen_op']).strip())), axis = 1)\n",
    "\n",
    "    if aid_subset != None:\n",
    "        df_subset = df.loc[df.AssignmentId.isin(aid_subset)]\n",
    "        df2 = df_subset.groupby(['AssignmentId', 'story_type'])['correct_pred'].sum()\n",
    "        acc = df_subset['correct_pred'].to_numpy().mean()\n",
    "        cons_acc = sum(df2.to_numpy() == 2)/len(df2)\n",
    "        print(f'For subset :: Accuracy :: {acc*100:.1f} Contrastive accuracy :: {cons_acc*100:.1f}')\n",
    "\n",
    "    df2 = df.groupby(['AssignmentId', 'story_type'])['correct_pred'].sum()\n",
    "    acc = df['correct_pred'].to_numpy().mean()\n",
    "    cons_acc = sum(df2.to_numpy() == 2)/len(df2)\n",
    "    print(f'Accuracy :: {acc*100:.1f} Contrastive accuracy :: {cons_acc*100:.1f}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human eval data - path list\n",
    "- Task 8 (Story-State Inference)\n",
    "  - human  : ```./../human_eval_data/mturk_op/MturkOP_Te_200_t8_1.csv``` (setting 1; default task setting)\n",
    "  - human  : ```./../human_eval_data/mturk_op/MturkOP_Te_200_t8.csv``` (setting 0; w/o justification set indicator)\n",
    "- Task 6\n",
    "  - T5-large : ```./../human_eval_data/mturk_op/MturkOP_Te_full_t_6_m_t5-large_b_4_lr_0.0001_w_1e-06_s_0_epoch_4.csv```\n",
    "  - T5-base : ```./../human_eval_data/mturk_op/MturkOP_Te_full_t_6_m_t5-base_b_12_lr_0.0001_w_1e-06_s_0_epoch_6.csv```\n",
    "  - GPT3: ```./../human_eval_data/mturk_op/MturkOP_Te_200_t6_GPT3_app_3_exs_10.csv```\n",
    "- Task 7\n",
    "  - T5-large: ```./../human_eval_data/mturk_op/MturkOP_Te_full_t_7_m_t5-large_b_4_lr_0.0001_w_1e-06_s_0_epoch_4.csv```\n",
    "  - T5-base: ```./../human_eval_data/mturk_op/MturkOP_Te_full_t_7_m_t5-base_b_10_lr_0.0001_w_1e-06_s_0_epoch_6.csv```\n",
    "  - GPT3: ```./../human_eval_data/mturk_op/MturkOP_Te_200_t7_GPT3_app_1_exs_5.csv```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :: 93.5 Contrastive accuracy :: 88.8\n",
      "Accuracy :: 96.2 Contrastive accuracy :: 93.0\n",
      "Accuracy :: 90.8 Contrastive accuracy :: 84.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24408/1845339323.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['correct_pred'] = df.apply(lambda x: int((str(x['target_op']).strip() == str(x['gen_op']).strip())), axis = 1)\n",
      "/tmp/ipykernel_24408/1845339323.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['correct_pred'] = df.apply(lambda x: int((str(x['target_op']).strip() == str(x['gen_op']).strip())), axis = 1)\n"
     ]
    }
   ],
   "source": [
    "task_8_human_response = pd.read_csv('./../human_eval_data/mturk_op/MturkOP_Te_200_t8.csv')\n",
    "req_cols = ['Input.AssignmentId',  'Input.Input_line1', 'Input.Input_line2', 'Input.Input_line3', 'Input.Input_line4', 'Input.Input_line5',\n",
    "            'Input.assertion', 'Input.story_state_flag', 'WorkerId',\n",
    "            'Answer.sb_entail_a.0', 'Answer.sb_entail_a.1', 'Answer.sb_entail_a.2', 'Answer.sb_entail_a.3', 'Answer.sb_entail_a.4']\n",
    "task_8_human_response = task_8_human_response.loc[:, req_cols]\n",
    "task_8_human_response.rename(columns={\"Input.AssignmentId\": \"AssignmentId\"}, inplace=True)\n",
    "\n",
    "task_8_human_response['story_type'] = task_8_human_response['Input.story_state_flag'].apply(lambda x: 'story' if x.startswith('story') == True else 'mod_story')\n",
    "task_8_human_response['state_type'] = task_8_human_response['Input.story_state_flag'].apply(lambda x: 'mod_state' if x.endswith('mod_state') == True else 'state')\n",
    "task_8_human_response['gen_op'] = task_8_human_response.apply(lambda x: [i for i in range(5) if x[f'Answer.sb_entail_a.{i}'] == True][0] >= 3 , axis=1)\n",
    "task_8_human_response['target_op'] = task_8_human_response['Input.story_state_flag'].apply(lambda x: x in ['mod_story_mod_state', 'story_state'])\n",
    "\n",
    "task_8_human_response2 = task_8_human_response.groupby(['AssignmentId', 'story_type', 'state_type', 'target_op'])['gen_op'].sum()>1\n",
    "task_8_human_response2 = task_8_human_response2.reset_index()\n",
    "get_acc(task_8_human_response2, aid_subset = None)\n",
    "get_acc(task_8_human_response2.loc[task_8_human_response2.story_type == 'story'], aid_subset = None)\n",
    "get_acc(task_8_human_response2.loc[task_8_human_response2.story_type == 'mod_story'], aid_subset = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Correct Implication score :: 41.0\n",
      ">> story is lgoical:: 77.0\n",
      ">> Both story is lgoical and implies the conterfactual state:: 34.0\n",
      ">> Similarity score:: 91.38888888888887\n"
     ]
    }
   ],
   "source": [
    "# human_eval_task_6 = pd.read_csv('./../human_eval_data/mturk_op/MturkOP_Te_full_t_6_m_t5-large_b_4_lr_0.0001_w_1e-06_s_0_epoch_4.csv')\n",
    "human_eval_task_6 = pd.read_csv('./../human_eval_data/mturk_op/MturkOP_Te_full_t_6_m_t5-base_b_12_lr_0.0001_w_1e-06_s_0_epoch_6.csv')\n",
    "human_eval_task_6['story_logical'] = human_eval_task_6.apply(lambda x: [i for i in range(2) if x[f'Answer.sb_is_logical.{i}'] == True][0] == 1, axis=1)\n",
    "human_eval_task_6['correct_implication'] = human_eval_task_6.apply(lambda x: [i for i in range(5) if x[f'Answer.sb_entail_a.{i}']==True ][0]>=3, axis=1)\n",
    "human_eval_task_6['similarity'] = human_eval_task_6.apply(lambda x: [i for i in range(4) if x[f'Answer.sb_sim_sa.{i}']==True ][0], axis=1)\n",
    "\n",
    "human_eval_task_6_grp = human_eval_task_6.groupby(['Input.AssignmentId']).agg({'story_logical': 'mean', 'correct_implication': 'mean'}).reset_index()\n",
    "\n",
    "human_eval_task_6_grp['story_logical'] = human_eval_task_6_grp['story_logical'].apply(lambda x: int(x > 0.5))\n",
    "human_eval_task_6_grp['correct_implication'] = human_eval_task_6_grp['correct_implication'].apply(lambda x: int(x > 0.5))\n",
    "human_eval_task_6_grp['logical_correct_implication'] = human_eval_task_6_grp.apply(lambda x: x['story_logical']*x['correct_implication'], axis=1)\n",
    "\n",
    "print(f'>> Correct Implication score :: {human_eval_task_6_grp[\"correct_implication\"].mean()*100}')\n",
    "print(f'>> story is lgoical:: {human_eval_task_6_grp[\"story_logical\"].mean()*100}')\n",
    "print(f'>> Both story is lgoical and implies the conterfactual state:: {human_eval_task_6_grp[\"logical_correct_implication\"].mean()*100}')\n",
    "\n",
    "# print(f'>> Similarity score:: {100 - human_eval_task_6[\"similarity\"].mean()*100}')\n",
    "print(f'>> Similarity score:: {((3 - human_eval_task_6[\"similarity\"] )/3).mean()*100}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Correct Implication score :: 50.000000\n",
      ">> story is lgoical:: 86.000000\n",
      ">> Both story is lgoical and implies the conterfactual state:: 48.5\n",
      ">> Similarity score:: 86.33333333333333\n"
     ]
    }
   ],
   "source": [
    "human_eval_task_6 = pd.read_csv('./../human_eval_data/mturk_op/MturkOP_Te_200_t6_GPT3_app_3_exs_10.csv')\n",
    "\n",
    "human_eval_task_6['story_logical'] = human_eval_task_6.apply(lambda x: [i for i in range(2) if x[f'Answer.sb_is_logical.{i}'] == True][0] == 0, axis=1)\n",
    "human_eval_task_6['correct_implication'] = human_eval_task_6.apply(lambda x: [i for i in range(5) if x[f'Answer.sb_entail_a.{i}']==True ][0]<=1, axis=1)\n",
    "\n",
    "human_eval_task_6['similarity'] = human_eval_task_6.apply(lambda x: [i for i in range(4) if x[f'Answer.sb_sim_sa.{i}']==True ][0], axis=1)\n",
    "\n",
    "human_eval_task_6_grp = human_eval_task_6.groupby(['Input.AssignmentId']).agg({'story_logical': 'mean', 'correct_implication': 'mean', 'similarity': 'mean'}).reset_index()\n",
    "\n",
    "human_eval_task_6_grp['story_logical'] = human_eval_task_6_grp['story_logical'].apply(lambda x: int(x > 0.5))\n",
    "human_eval_task_6_grp['correct_implication'] = human_eval_task_6_grp['correct_implication'].apply(lambda x: int(x > 0.5))\n",
    "human_eval_task_6_grp['logical_correct_implication'] = human_eval_task_6_grp.apply(lambda x: x['story_logical']*x['correct_implication'], axis=1)\n",
    "\n",
    "# print(human_eval_task_6_grp['story_logical'].mean(), human_eval_task_6_grp['correct_implication'].mean(), human_eval_task_6_grp['logical_correct_implication'].mean())\n",
    "\n",
    "correct_implication = np.append(human_eval_task_6_grp[\"correct_implication\"].to_numpy(), np.zeros(200 - len(human_eval_task_6_grp[\"correct_implication\"])))\n",
    "logical_story = np.append(human_eval_task_6_grp[\"story_logical\"].to_numpy(), np.zeros(200 - len(human_eval_task_6_grp[\"story_logical\"])))\n",
    "logical_and_corr_impl = np.append(human_eval_task_6_grp[\"logical_correct_implication\"].to_numpy(), np.zeros(200 - len(human_eval_task_6_grp[\"logical_correct_implication\"])))\n",
    "similarity = np.append(human_eval_task_6[\"similarity\"].to_numpy(), np.zeros(600 - len(human_eval_task_6[\"similarity\"])))\n",
    "\n",
    "print(f'>> Correct Implication score :: {correct_implication.mean()*100:3f}')\n",
    "print(f'>> story is lgoical:: {logical_story.mean()*100:3f}')\n",
    "print(f'>> Both story is lgoical and implies the conterfactual state:: {logical_and_corr_impl.mean()*100}')\n",
    "print(f'>> Similarity score:: {((3-similarity)/3).mean()*100}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for both T5 and GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> state_is_attr\n",
      "99.25\n",
      "\n",
      ">> correct_state_change\n",
      "58.74999999999999\n",
      "\n",
      ">> state_not_in_story\n",
      "97.0\n",
      "\n",
      ">> overall_score\n",
      "55.49999999999999\n"
     ]
    }
   ],
   "source": [
    "# human_eval_task_7 = pd.read_csv('./../human_eval_data/mturk_op/MturkOP_Te_full_t_7_m_t5-base_b_10_lr_0.0001_w_1e-06_s_0_epoch_6.csv')\n",
    "human_eval_task_7 = pd.read_csv('./../human_eval_data/mturk_op/MturkOP_Te_full_t_7_m_t5-large_b_4_lr_0.0001_w_1e-06_s_0_epoch_4.csv')\n",
    "# human_eval_task_7 = pd.read_csv('./../human_eval_data/mturk_op/MturkOP_Te_200_t7_GPT3_app_1_exs_5.csv')\n",
    "\n",
    "human_eval_task_7['state_is_attr'] = human_eval_task_7.apply(lambda x: [i for i in range(2) if x[f'Answer.ass1_is_attr.{i}'] ==True ][0], axis=1)\n",
    "human_eval_task_7['state_not_in_story'] = human_eval_task_7['Answer.ass_in_story.0'].apply(lambda x: int(x == True))\n",
    "human_eval_task_7['state_imp_by_storya'] = human_eval_task_7.apply(lambda x: [i for i in range(5) if x[f'Answer.sa_entail_a.{i}'] ==True ][0] >= 3, axis=1)\n",
    "human_eval_task_7['state_imp_by_storyb'] = human_eval_task_7.apply(lambda x: [i for i in range(5) if x[f'Answer.sb_entail_a.{i}'] ==True ][0] >= 3, axis=1)\n",
    "\n",
    "human_eval_task_7_state = human_eval_task_7.loc[human_eval_task_7['Input.state_type'] == 'state', :]\n",
    "human_eval_task_7_mod_state = human_eval_task_7.loc[human_eval_task_7['Input.state_type'] == 'mod_state', :]\n",
    "\n",
    "human_eval_task_7_state_grp = human_eval_task_7_state.groupby('Input.AssignmentId').agg({'state_is_attr': 'mean', 'state_not_in_story': 'mean', 'state_imp_by_storya': 'mean', 'state_imp_by_storyb': 'mean'}).reset_index()\n",
    "human_eval_task_7_mod_state_grp = human_eval_task_7_mod_state.groupby('Input.AssignmentId').agg({'state_is_attr': 'mean', 'state_not_in_story': 'mean', 'state_imp_by_storya': 'mean', 'state_imp_by_storyb': 'mean'}).reset_index()\n",
    "\n",
    "human_eval_task_7_state_grp['state_is_attr'] = human_eval_task_7_state_grp['state_is_attr'].apply(lambda x: int(x > 0.5))\n",
    "human_eval_task_7_mod_state_grp['state_is_attr'] = human_eval_task_7_mod_state_grp['state_is_attr'].apply(lambda x: int(x > 0.5))\n",
    "\n",
    "human_eval_task_7_state_grp['state_not_in_story'] = human_eval_task_7_state_grp['state_not_in_story'].apply(lambda x: int(x > 0.5))\n",
    "human_eval_task_7_mod_state_grp['state_not_in_story'] = human_eval_task_7_mod_state_grp['state_not_in_story'].apply(lambda x: int(x > 0.5))\n",
    "\n",
    "human_eval_task_7_state_grp['correct_state_change'] = human_eval_task_7_state_grp.apply(lambda x: np.max(x['state_imp_by_storya'] - x['state_imp_by_storyb'], 0), axis=1)\n",
    "human_eval_task_7_mod_state_grp['correct_state_change'] = human_eval_task_7_mod_state_grp.apply(lambda x: np.max(x['state_imp_by_storyb'] - x['state_imp_by_storya'], 0), axis=1)\n",
    "\n",
    "human_eval_task_7_state_grp['overall_score'] = human_eval_task_7_state_grp.apply(lambda x: x['state_is_attr']*x['state_not_in_story']*x['correct_state_change'], axis=1)\n",
    "human_eval_task_7_mod_state_grp['overall_score'] = human_eval_task_7_mod_state_grp.apply(lambda x: x['state_is_attr']*x['state_not_in_story']*x['correct_state_change'], axis=1)\n",
    "\n",
    "col = ['state_is_attr', 'correct_state_change', 'state_not_in_story', 'overall_score']\n",
    "for c in col:\n",
    "    print(f'\\n>> {c}')\n",
    "    # var = (human_eval_task_7_state_grp[c].to_numpy()*human_eval_task_7_mod_state_grp[c].to_numpy()).mean()\n",
    "    var = (human_eval_task_7_state_grp[c].mean() + human_eval_task_7_mod_state_grp[c].mean())/2\n",
    "    print(var*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0edde771779a501cfe53ce2213f1048a933c3e3c4c44d33ef48d8a85fbd34224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
