{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openai\n",
    "from copy import deepcopy as cc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "bert_score = evaluate.load(\"bertscore\")\n",
    "bleu_score = evaluate.load(\"google_bleu\")\n",
    "bleurt = evaluate.load(\"bleurt\", module_type=\"metric\")\n",
    "openai.api_key = \"xxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_ip = pd.read_csv('./../Data/human_eval_dat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_selected_AID = ['3AQF3RZ5596OVGWAPKGAIYKWIISF63', '3K9FOBBF2I7I2TCYAC5PFMB2AV3NL8', '37XITHEISXXAENAKQ6T7LNQPHKWCRP', '3ERET4BTVNXMTLQMO2AJIL33GJP9KQ', '3OB0CAO74IDASWXMQTZ24AG2XAVYHN',\n",
    "                       '386PBUZZXGLMJ3LDVEDXMFUL1R9JL7', '3E7TUJ2EGDAEFFKCWLOWE88DWVX9DO', '3K9FOBBF2I7I2TCYAC5PFMB2AZGNLT', '3II4UPYCOKVK7E1YDNZI03Y26BMDQV', '36PW28KO40KXC48HKMDGGL1I2C8AEZ',\n",
    "                       '39JEC7537VP27UR1ZQYHMKFS29PVCJ', '3CFJTT4SXUER97C592RNR9XKTVGI7S', '3JPSL1DZ5TN16ALUDLGZ68VRG36ANJ', '3OF2M9AATHC1ZC8ZS04IV95WVUAKZT', '30LB5CDZNDYZMM1VO7U0CPQNHV10ZY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = {'8': \"Task Description: Can the 'State' be inferred from the 'Story' ?\", \n",
    "                    '6': \"Task Description: Make minimal revision to the 'Story' to make it consistent with the 'State'.\", \n",
    "                    '7': \"Task Description: Describe the change in participant state between 'Story 1' and 'Story 2'.\"}\n",
    "\n",
    "gpt3_dat = pd.read_pickle(\"./../Data/GPT3_roberta_repr.pkl\") \n",
    "\n",
    "gpt3_dat['task_6'] = gpt3_dat.apply(lambda x: f\"Story: {x['Story']}\\nState: {x['mod_assertion']}\\nRevised Story: {x['Mod_Story']}\\n\", axis=1)\n",
    "gpt3_dat['task_7'] = gpt3_dat.apply(lambda x: f\"Story 1: {x['Story']}\\nStory 2: {x['Mod_Story']}\\nState 1: {x['assertion']}\\nState 2: {x['mod_assertion']}\\n\", axis=1)\n",
    "\n",
    "gpt3_dat['task_6_query'] = gpt3_dat.apply(lambda x: f\"Story: {x['Story']}\\nState: {x['mod_assertion']}\\nRevised Story: \", axis=1)\n",
    "gpt3_dat['task_7_query'] = gpt3_dat.apply(lambda x: f\"Story 1: {x['Story']}\\nStory 2: {x['Mod_Story']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_len = val_dat.shape[0]\n",
    "# val_sample = np.random.permutation(np.arange(val_len))[:200]\n",
    "# np.save('./../Data/GPT3_val_sample.npy', val_sample)\n",
    "# val_sample = np.load('./../Data/GPT3_val_sample.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8476, 17), (202, 17), (200, 17))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete training data\n",
    "tr_dat = gpt3_dat.loc[gpt3_dat.dat_type == 'Train_set']\n",
    "\n",
    "# Subset of training data, that are curated by expert (w/o noise)\n",
    "tr_dat_curated = gpt3_dat.loc[gpt3_dat.apply(lambda x: (x.dat_type == 'Train_set') and (x.GPT3_subset == 1.0), axis = 1), :]\n",
    "\n",
    "# Validation data - used for hyperparam selection\n",
    "# val_dat = gpt3_dat.loc[gpt3_dat.dat_type == 'Val_set']\n",
    "# val_dat = val_dat.iloc[val_sample]\n",
    "\n",
    "# Evaluation/test data - used for reporting GPT3 performance\n",
    "val_dat = gpt3_dat.loc[gpt3_dat.AssignmentId.isin(human_eval_ip.AssignmentId)]\n",
    "\n",
    "tr_dat.shape, tr_dat_curated.shape, val_dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the subsequent cells:\n",
    "- task_no = 6 for story generation from counterfactual\n",
    "- task_no = 7 for state change generation task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 - Expert curated prompt examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_exs_list = [10]\n",
    "approach = 1\n",
    "task_no = 6\n",
    "for no_of_exs in no_of_exs_list:\n",
    "    task_prompt = \"\\n\".join(gpt3_dat.loc[gpt3_dat.AssignmentId.isin(expert_selected_AID[:no_of_exs]), f'task_{task_no}'].tolist())\n",
    "    task_prompt = task_description['6'] + '\\n\\n' + task_prompt\n",
    "\n",
    "    final_dict_task = []\n",
    "    for idx, row in tqdm(val_dat.iloc[:].iterrows(), total=len(val_dat.iloc[:])):\n",
    "        df_temp = row.to_dict()\n",
    "        gpt3_query = task_prompt + '\\n' + df_temp[f'task_{task_no}_query']\n",
    "        response = openai.Completion.create(model=\"text-davinci-002\",\n",
    "                                                prompt=gpt3_query,\n",
    "                                                temperature=.9,\n",
    "                                                max_tokens=100,\n",
    "                                                top_p=1,\n",
    "                                                frequency_penalty=0.5,\n",
    "                                                presence_penalty=0.1)\n",
    "        df_temp['GPT3_response'] = response['choices']\n",
    "        df_temp['GPT3_response_readable'] = response['choices'][0]['text'].strip()\n",
    "        final_dict_task.append(df_temp)\n",
    "\n",
    "    final_dict_task_copy = pd.DataFrame(final_dict_task)\n",
    "    final_dict_task_copy.to_csv(f'./../gpt3_op/Val_approach_{approach}_task_{task_no}_{no_of_exs}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 - Randomly sampled incontext examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_exs_list = [10]\n",
    "task_no = 6\n",
    "approach = 2\n",
    "\n",
    "for no_of_exs in no_of_exs_list:\n",
    "    final_dict_task = []\n",
    "    for idx, row in tqdm(val_dat.iloc[:].iterrows(), total=len(val_dat.iloc[:])):\n",
    "\n",
    "        # If we want to create prompt from examples randomly sampled from the entire train set, then use the 1st snippet below\n",
    "        # task_prompt = \"\\n\".join(tr_dat.sample(no_of_exs).loc[:, f'task_{task_no}'].tolist())\n",
    "\n",
    "        # If we want to create prompt from examples randomly sampled from the subset of 200 expert curated train set, then use the 2nd snippet below            \n",
    "        task_prompt = \"\\n\".join(tr_dat_curated.sample(no_of_exs).loc[:, f'task_{task_no}'].tolist())\n",
    "        \n",
    "        task_prompt = task_description[f'{task_no}'] + '\\n\\n' + task_prompt\n",
    "\n",
    "        df_temp = row.to_dict()\n",
    "        gpt3_query = task_prompt + '\\n' + df_temp[f'task_{task_no}_query']\n",
    "        response = openai.Completion.create(model=\"text-davinci-002\",\n",
    "                                                prompt=gpt3_query,\n",
    "                                                temperature=.9,\n",
    "                                                max_tokens=100,\n",
    "                                                top_p=1,\n",
    "                                                frequency_penalty=0.5,\n",
    "                                                presence_penalty=0.1)\n",
    "        df_temp['GPT3_response'] = response['choices']\n",
    "        df_temp['GPT3_prompt'] = gpt3_query\n",
    "\n",
    "        df_temp['GPT3_response_readable'] = response['choices'][0]['text'].strip()\n",
    "        final_dict_task.append(df_temp)\n",
    "\n",
    "    final_dict_task_copy = pd.DataFrame(final_dict_task)\n",
    "    final_dict_task_copy.to_csv(f'./../gpt3_op/Val_approach_{approach}_task_{task_no}_{no_of_exs}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3 (Nearest neighbor - RoBERTa similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "cos = nn.CosineSimilarity(dim=1, eps=0)\n",
    "\n",
    "def get_similarity_task6(k, search_df, query_row):\n",
    "\n",
    "    query_story_vec = query_row['story_Roberta_repr'].unsqueeze(0)\n",
    "    query_conterfactual_state_vec = query_row['mod_state_Roberta_repr'].unsqueeze(0)\n",
    "\n",
    "    candidate_story_vec = search_df['story_Roberta_repr']\n",
    "    candidate_conterfactual_state_vec = search_df['mod_state_Roberta_repr']\n",
    "\n",
    "    candidate_story_vec = torch.cat(list(map(lambda x: x.unsqueeze(0), candidate_story_vec)))\n",
    "    candidate_conterfactual_state_vec = torch.cat(list(map(lambda x: x.unsqueeze(0), candidate_conterfactual_state_vec)))\n",
    "\n",
    "    story_similarity = cos(query_story_vec, candidate_story_vec)\n",
    "    state_similarity = cos(query_conterfactual_state_vec, candidate_conterfactual_state_vec)\n",
    "\n",
    "    total_similarity = (story_similarity + state_similarity)/2\n",
    "    most_similar_indices = total_similarity.argsort()[-k::]\n",
    "\n",
    "    return most_similar_indices.numpy()\n",
    "\n",
    "\n",
    "def get_similarity_task7(k, search_df, query_row):\n",
    "\n",
    "    query_story_vec = query_row['story_Roberta_repr'].unsqueeze(0)\n",
    "    query_mod_story_vec = query_row['mod_story_Roberta_repr'].unsqueeze(0)\n",
    "\n",
    "    candidate_story_vec = search_df['story_Roberta_repr']\n",
    "    candidate_mod_story_vec = search_df['mod_state_Roberta_repr']\n",
    "\n",
    "    candidate_story_vec = torch.cat(list(map(lambda x: x.unsqueeze(0), candidate_story_vec)))\n",
    "    candidate_mod_story_vec = torch.cat(list(map(lambda x: x.unsqueeze(0), candidate_mod_story_vec)))\n",
    "\n",
    "    story_similarity = cos(query_story_vec, candidate_story_vec)\n",
    "    mod_story_similarity = cos(query_mod_story_vec, candidate_mod_story_vec)\n",
    "\n",
    "    total_similarity = (story_similarity + mod_story_similarity)/2\n",
    "    most_similar_indices = total_similarity.argsort()[-k::]\n",
    "\n",
    "    return most_similar_indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_exs_list = [10]\n",
    "task_no = 6\n",
    "approach = 3\n",
    "\n",
    "for no_of_exs in no_of_exs_list:\n",
    "    final_dict_task = []\n",
    "\n",
    "    for idx, row in tqdm(val_dat.iloc[:].iterrows(), total=len(val_dat.iloc[:])):\n",
    "\n",
    "        if task_no == 6:\n",
    "            nearest_k_instances_idx = get_similarity_task6(k = no_of_exs, search_df=tr_dat, query_row=row)\n",
    "        if task_no == 7:\n",
    "            nearest_k_instances_idx = get_similarity_task7(k = no_of_exs, search_df=tr_dat, query_row=row)\n",
    "\n",
    "        task_prompt = \"\\n\".join(tr_dat.iloc[nearest_k_instances_idx][f'task_{task_no}'].tolist())\n",
    "        task_prompt = task_description[f'{task_no}'] + '\\n\\n' + task_prompt\n",
    "\n",
    "        df_temp = row.to_dict()\n",
    "        gpt3_query = task_prompt + '\\n' + df_temp[f'task_{task_no}_query']\n",
    "        response = openai.Completion.create(model=\"text-davinci-002\",\n",
    "                                                prompt=gpt3_query,\n",
    "                                                temperature=.9,\n",
    "                                                max_tokens=100,\n",
    "                                                top_p=1,\n",
    "                                                frequency_penalty=0.5,\n",
    "                                                presence_penalty=0.1)\n",
    "        df_temp['GPT3_response'] = response['choices']\n",
    "        df_temp['GPT3_prompt'] = gpt3_query\n",
    "        df_temp['GPT3_response_readable'] = response['choices'][0]['text'].strip()\n",
    "        final_dict_task.append(df_temp)\n",
    "    final_dict_task_copy = pd.DataFrame(final_dict_task)\n",
    "    final_dict_task_copy.to_csv(f'./../gpt3_op/Hu_eval_approach_{approach}_task_{task_no}_{no_of_exs}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Hyperparameter selection: Automatic evaluation for \n",
    "     - 3 prompting approaches (random, expert-curated, most similar in-context examples)\n",
    "     -  different # of incontext examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval of Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = [1, 2, 3]\n",
    "no_of_exs_list = [5, 10, 15]\n",
    "task_no = 7\n",
    "\n",
    "for approach in approaches:\n",
    "    print(f'\\n{\"--\"*20}\\nApproach number :: {approach}\\n{\"--\"*20}\\n')\n",
    "    for no_of_exs in no_of_exs_list:\n",
    "        print(f'\\n>> number of examples in prompt :: {no_of_exs}')\n",
    "\n",
    "        final_dict_task_copy = pd.read_csv(f'./../gpt3_op/Val_approach_{approach}_task_{task_no}_{no_of_exs}.csv')\n",
    "        final_dict_task_copy['task7_ref'] = final_dict_task_copy.apply(lambda x: 'State 1: ' + x['assertion'] + ' State 2: ' + x['mod_assertion'], axis = 1)\n",
    "        final_dict_task_copy['GPT3_response_readable'] = final_dict_task_copy['GPT3_response_readable'].apply(lambda x: x.strip().replace('\\n', ' '))\n",
    "\n",
    "        bleu_3 = bleu_score.compute(predictions=final_dict_task_copy['GPT3_response_readable'], references = final_dict_task_copy['Mod_Story'], max_len = 4, min_len = 1)\n",
    "        bert_s = bert_score.compute(predictions=final_dict_task_copy['GPT3_response_readable'], references=final_dict_task_copy['Mod_Story'], lang = 'en', rescale_with_baseline = True)\n",
    "        bleurt_s = bleurt.compute(predictions=final_dict_task_copy['GPT3_response_readable'], references=final_dict_task_copy['Mod_Story'])\n",
    "\n",
    "        print(np.mean(bleurt_s['scores']), np.mean(bert_s['f1']), bleu_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eval of Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "Approach number :: 1\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      ">> number of examples in prompt :: 5\n",
      "-0.7145838774275035 0.5299286359548568 0.10743231918681027\n",
      "\n",
      ">> number of examples in prompt :: 10\n",
      "-0.7674064121022821 0.5280463938601314 0.10857974253511266\n",
      "\n",
      ">> number of examples in prompt :: 15\n",
      "-0.7546867879386991 0.519381995536387 0.10727671918245689\n",
      "\n",
      "----------------------------------------\n",
      "Approach number :: 2\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      ">> number of examples in prompt :: 5\n",
      "-0.834946702774614 0.5143468851875515 0.09402387377308594\n",
      "\n",
      ">> number of examples in prompt :: 10\n",
      "-0.7845454420521856 0.5121545459702611 0.09883878876225816\n",
      "\n",
      ">> number of examples in prompt :: 15\n",
      "-0.786708257496357 0.5216857675660866 0.10466752868676693\n",
      "\n",
      "----------------------------------------\n",
      "Approach number :: 3\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      ">> number of examples in prompt :: 5\n",
      "-0.7491250667441636 0.5210992804542184 0.10971820557996129\n",
      "\n",
      ">> number of examples in prompt :: 10\n",
      "-0.8096398518886417 0.5141855759173631 0.10328679004768423\n",
      "\n",
      ">> number of examples in prompt :: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7900637151300907 0.5027628561481834 0.10533783783783784\n"
     ]
    }
   ],
   "source": [
    "approaches = [1, 2, 3]\n",
    "no_of_exs_list = [5, 10, 15]\n",
    "task_no = 7\n",
    "\n",
    "for approach in approaches:\n",
    "    print(f'\\n{\"--\"*20}\\nApproach number :: {approach}\\n{\"--\"*20}\\n')\n",
    "    for no_of_exs in no_of_exs_list:\n",
    "        print(f'\\n>> number of examples in prompt :: {no_of_exs}')\n",
    "        final_dict_task_copy = pd.read_csv(f'./../gpt3_op/Val_approach_{approach}_task_{task_no}_{no_of_exs}.csv')\n",
    "\n",
    "        # final_dict_task_copy = pd.read_csv(f'./../gpt3_op/Val_approach_{2}_task_{7}_{10}.csv')\n",
    "        # final_dict_task_copy['task7_ref_state1'] = final_dict_task_copy.apply(lambda x: 'State 1: ' + x['assertion'] + ' State 2: ' + x['mod_assertion'], axis = 1)\n",
    "        final_dict_task_copy['GPT3_response_readable'] = final_dict_task_copy['GPT3_response_readable'].apply(lambda x: str(x).strip().replace('\\n', ' '))\n",
    "        final_dict_task_copy['GPT3_response_readable_state1'] = final_dict_task_copy['GPT3_response_readable'].apply(lambda x: x.split('State 2:')[0].replace('State 1:', '').strip())\n",
    "        final_dict_task_copy['GPT3_response_readable_state2'] = final_dict_task_copy['GPT3_response_readable'].apply(lambda x: x.split('State 2:')[-1].strip())\n",
    "\n",
    "        bleu_3_state1 = bleu_score.compute(predictions=final_dict_task_copy['GPT3_response_readable_state1'], references = final_dict_task_copy['assertion'], max_len = 4, min_len = 1)\n",
    "        bert_s_state1 = bert_score.compute(predictions=final_dict_task_copy['GPT3_response_readable_state1'], references=final_dict_task_copy['assertion'], lang = 'en', rescale_with_baseline = True)\n",
    "        bleurt_s_state1 = bleurt.compute(predictions=final_dict_task_copy['GPT3_response_readable_state1'], references=final_dict_task_copy['assertion'])\n",
    "\n",
    "        bleu_3_state2 = bleu_score.compute(predictions=final_dict_task_copy['GPT3_response_readable_state2'], references = final_dict_task_copy['mod_assertion'], max_len = 4, min_len = 1)\n",
    "        bert_s_state2 = bert_score.compute(predictions=final_dict_task_copy['GPT3_response_readable_state2'], references=final_dict_task_copy['mod_assertion'], lang = 'en', rescale_with_baseline = True)\n",
    "        bleurt_s_state2 = bleurt.compute(predictions=final_dict_task_copy['GPT3_response_readable_state2'], references=final_dict_task_copy['mod_assertion'])\n",
    "\n",
    "        print((np.mean(bleurt_s_state1['scores']) + np.mean(bleurt_s_state2['scores']))/2, (np.mean(bert_s_state1['f1']) + np.mean(bert_s_state2['f1']))/2, (bleu_3_state1['google_bleu'] + bleu_3_state2['google_bleu'])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Generating RoBERTa representation for story/states for each pasta instance [Don't care/ignore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaModel,\n",
    "    RobertaTokenizerFast)\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-large') \n",
    "roberta_chkpt = torch.load('./../model_checkpoint_TACL3/t_8_1_m_roberta-large_b_12_lr_5e-06_w_1e-06_s_0_epoch_4.pt', map_location=torch.device('cpu'))\n",
    "roberta_chkpt = OrderedDict([(k.replace('LM_base.', ''), v) for k, v in roberta_chkpt.items() if k.startswith('linear_op') ==  False])\n",
    "roberta_mdl = RobertaModel.from_pretrained('roberta-large')\n",
    "roberta_mdl.load_state_dict(roberta_chkpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_tok = tokenizer.batch_encode_plus(gpt3_dat['Story'].tolist(), padding=True)\n",
    "state_tok = tokenizer.batch_encode_plus(gpt3_dat['assertion'].tolist(), padding=True)\n",
    "mod_story_tok = tokenizer.batch_encode_plus(gpt3_dat['mod_assertion'].tolist(), padding=True)\n",
    "mod_story_tok = tokenizer.batch_encode_plus(gpt3_dat['Mod_Story'].tolist(), padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = roberta_mdl.forward(input_ids=torch.tensor(res['input_ids']), attention_mask=torch.tensor(res['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 12\n",
    "st, end = 0, gpt3_dat.shape[0]\n",
    "ix_list = np.arange(st, end, B)\n",
    "\n",
    "st_ix = ix_list\n",
    "end_ix = np.append(ix_list[1:], end)\n",
    "\n",
    "story_repr, state_repr, mod_state_repr, mod_story_repr = [], [], [], []\n",
    "\n",
    "for s, e in tqdm(zip(st_ix, end_ix), total = len(st_ix)):\n",
    "    rows_batch = gpt3_dat.iloc[s: e]\n",
    "\n",
    "    story_tok = tokenizer.batch_encode_plus(rows_batch['Story'].tolist(), padding=True)\n",
    "    state_tok = tokenizer.batch_encode_plus(rows_batch['assertion'].tolist(), padding=True)\n",
    "    mod_state_tok = tokenizer.batch_encode_plus(rows_batch['mod_assertion'].tolist(), padding=True)\n",
    "    mod_story_tok = tokenizer.batch_encode_plus(rows_batch['Mod_Story'].tolist(), padding=True)\n",
    "\n",
    "    story_batch_repr = roberta_mdl.forward(input_ids=torch.tensor(story_tok['input_ids']), attention_mask=torch.tensor(story_tok['attention_mask']))\n",
    "    state_batch_repr = roberta_mdl.forward(input_ids=torch.tensor(state_tok['input_ids']), attention_mask=torch.tensor(state_tok['attention_mask']))\n",
    "    mod_state_batch_repr = roberta_mdl.forward(input_ids=torch.tensor(mod_state_tok['input_ids']), attention_mask=torch.tensor(mod_state_tok['attention_mask']))\n",
    "    mod_story_batch_repr = roberta_mdl.forward(input_ids=torch.tensor(mod_story_tok['input_ids']), attention_mask=torch.tensor(mod_story_tok['attention_mask']))\n",
    "\n",
    "    story_repr.extend(story_batch_repr['last_hidden_state'][:, 0, :])\n",
    "    state_repr.extend(state_batch_repr['last_hidden_state'][:, 0, :])\n",
    "    mod_state_repr.extend(mod_state_batch_repr['last_hidden_state'][:, 0, :])\n",
    "    mod_story_repr.extend(mod_story_batch_repr['last_hidden_state'][:, 0, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0edde771779a501cfe53ce2213f1048a933c3e3c4c44d33ef48d8a85fbd34224"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
